{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL8rRKVdSgfe"
      },
      "source": [
        "**20 newsgroups' dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXvEWYlNsYX"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "X_test, y_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "X_train, y_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAPuxYRMSl7o"
      },
      "source": [
        "**Reuter's dataset**\n",
        "\n",
        "\n",
        "*   Loading the data and parsing it\n",
        "*   All the numbers/dates can be removed, since they are not important\n",
        "*   The punctuation is also being filtered out by regex\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_7qIxU20Fs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3897cb-d31a-4b6e-bd70-b9abc3126a96"
      },
      "source": [
        "!wget -N http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
        "!tar zxf reuters21578.tar.gz\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-04 20:07:24--  http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘reuters21578.tar.gz’ not modified on server. Omitting download.\n",
            "\n",
            "all-exchanges-strings.lc.txt\t    reut2-002.sgm  reut2-013.sgm\n",
            "all-orgs-strings.lc.txt\t\t    reut2-003.sgm  reut2-014.sgm\n",
            "all-people-strings.lc.txt\t    reut2-004.sgm  reut2-015.sgm\n",
            "all-places-strings.lc.txt\t    reut2-005.sgm  reut2-016.sgm\n",
            "all-topics-strings.lc.txt\t    reut2-006.sgm  reut2-017.sgm\n",
            "cat-descriptions_120396.txt\t    reut2-007.sgm  reut2-018.sgm\n",
            "feldman-cia-worldfactbook-data.txt  reut2-008.sgm  reut2-019.sgm\n",
            "lewis.dtd\t\t\t    reut2-009.sgm  reut2-020.sgm\n",
            "README.txt\t\t\t    reut2-010.sgm  reut2-021.sgm\n",
            "reut2-000.sgm\t\t\t    reut2-011.sgm  reuters21578.tar.gz\n",
            "reut2-001.sgm\t\t\t    reut2-012.sgm  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_cmfmeYN5Iv",
        "outputId": "a27462ac-d4a5-4126-ff41-d77a1ca69b97"
      },
      "source": [
        "#loading the data\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "cwd = '/content/'\n",
        "files = os.listdir(cwd)\n",
        "\n",
        "raw_y = []\n",
        "raw_X = []\n",
        "\n",
        "for file in files:\n",
        "  if file.startswith(\"reut2-\") and file.endswith(\".sgm\"):\n",
        "\n",
        "    with open(file, 'rb') as f:\n",
        "\n",
        "      print('Reading file: %s' % file)\n",
        "\n",
        "      soup = BeautifulSoup(f, \"html.parser\")  #\"lxml\"\n",
        "      articles = soup.find_all('reuters')\n",
        "\n",
        "      for article in articles:\n",
        "        topics = article.findAll('topics')\n",
        "        bodies = article.findAll('text')\n",
        "\n",
        "        if len(topics[0]) == 0:\n",
        "          topics_list = ''  #no topic\n",
        "        else:\n",
        "          topics_list = []\n",
        "          for topic in topics[0]:  #getting rid of the tags \n",
        "            topics_list.append((topic.text).lower())\n",
        "        raw_y.append(topics_list)\n",
        "        \n",
        "        if len(bodies) == 0:\n",
        "          raw_X.append('')\n",
        "        else:\n",
        "          body = (bodies[0].text).lower()\n",
        "          body = re.sub('[^A-Za-z0-9]+', ' ', body) #getting rid of special char\n",
        "          body = re.sub(r'\\d+','nn', body)  #getting rid of numbers\n",
        "          raw_X.append(body)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-014.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-020.sgm\n",
            "Reading file: reut2-015.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-009.sgm\n",
            "Reading file: reut2-001.sgm\n",
            "Reading file: reut2-021.sgm\n",
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-008.sgm\n",
            "Reading file: reut2-005.sgm\n",
            "Reading file: reut2-004.sgm\n",
            "Reading file: reut2-000.sgm\n",
            "Reading file: reut2-007.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-018.sgm\n",
            "Reading file: reut2-002.sgm\n",
            "Reading file: reut2-003.sgm\n",
            "Reading file: reut2-013.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoFdsb1QSbk1"
      },
      "source": [
        "Vectorizer \n",
        "\n",
        "\n",
        "*   stop words are being removed by the vectorizer\n",
        "*   3-gram tokens\n",
        "\n",
        "We also need a Multi-Label attribute, since an article can have more than one topic. Therefore I'm taking multiLabelBinarizer for our topics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dl5LQ_cTWaB"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNTexfBzOBDI"
      },
      "source": [
        "# Splitting the data X and y into training and testing sets with the testing size\n",
        "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size = 0.2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7OAkOpXUSrU"
      },
      "source": [
        "#vectorizer = CountVectorizer(analyzer='word',stop_words=\"english\", ngram_range=(1, 3))\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 3))\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "#print(vectorizer.get_feature_names()[0:100])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5xbUcGsPyuz"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer  \n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(y_train) \n",
        "y_test = mlb.transform(y_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ6x1ofHP5Dp",
        "outputId": "6976d371-9cb5-49e2-9e50-9aa6423b43c3"
      },
      "source": [
        "assert(X_train.shape[0]==y_train.shape[0])    #to make sure dataset is loaded correctly\n",
        "assert(X_test.shape[0]==y_test.shape[0])\n",
        "print(f'Number of Training Samples: {y_train.shape[0]}')\n",
        "print(f'Number of Test Samples: {y_test.shape[0]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Samples: 17262\n",
            "Number of Test Samples: 4316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei0yiJUAMrHj"
      },
      "source": [
        "# Non-probabilistic classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJSYIZBeVW6O"
      },
      "source": [
        "\n",
        "\n",
        "1.   Trying to find the best hyperparameters (before running the first kernel, take into consideration that the code will run for at least 40 minutes).\n",
        "2.   Training and testing with the best hyperparameters (parameters are taken from the sklearn documentation).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTlVo0tVZtbj"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAipY9JSB3hs",
        "outputId": "4863dcf8-a994-429e-8968-8492d30b9aa8"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "#parameters\n",
        "loss_parameters = ['hinge', 'modified_huber', 'squared_hinge', 'perceptron']\n",
        "penalty_parameters = ['l2', 'l1', 'elasticnet']\n",
        "alpha_parameters = [1e-5, 0.0001, 0.001, 0.01]\n",
        "\n",
        "#dataframe\n",
        "results = []\n",
        "\n",
        "#testing the best parameters \n",
        "for l in loss_parameters:\n",
        "  for p in penalty_parameters:\n",
        "    for a in alpha_parameters: \n",
        "      with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")  #ignoring UserWarning \n",
        "\n",
        "        # Classifier\n",
        "        classifier = OneVsRestClassifier(SGDClassifier(alpha=a, penalty=p, loss=l)) \n",
        "\n",
        "        # train\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        print(f'Training with hyperparameters: alpha [{a}], loss [{l}], penalty [{p}]')\n",
        "\n",
        "        #predict\n",
        "        predictions = classifier.predict(X_test)\n",
        "\n",
        "        #accuracy\n",
        "        accuracy = accuracy_score(y_test, predictions)*100\n",
        "\n",
        "\n",
        "        #dataframe\n",
        "        results.append({\n",
        "            'Alpha': a,\n",
        "            'Penalty': p,\n",
        "            'Loss': l,\n",
        "            'Accuracy': round(accuracy, 3)\n",
        "              })"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: alpha [1e-05], loss [hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.0001], loss [hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.001], loss [hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.01], loss [hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [1e-05], loss [hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.0001], loss [hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.001], loss [hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.01], loss [hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [1e-05], loss [hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.0001], loss [hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.001], loss [hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.01], loss [hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [1e-05], loss [modified_huber], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.0001], loss [modified_huber], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.001], loss [modified_huber], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.01], loss [modified_huber], penalty [l2]\n",
            "Training with hyperparameters: alpha [1e-05], loss [modified_huber], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.0001], loss [modified_huber], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.001], loss [modified_huber], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.01], loss [modified_huber], penalty [l1]\n",
            "Training with hyperparameters: alpha [1e-05], loss [modified_huber], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.0001], loss [modified_huber], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.001], loss [modified_huber], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.01], loss [modified_huber], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [1e-05], loss [squared_hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.0001], loss [squared_hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.001], loss [squared_hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.01], loss [squared_hinge], penalty [l2]\n",
            "Training with hyperparameters: alpha [1e-05], loss [squared_hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.0001], loss [squared_hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.001], loss [squared_hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.01], loss [squared_hinge], penalty [l1]\n",
            "Training with hyperparameters: alpha [1e-05], loss [squared_hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.0001], loss [squared_hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.001], loss [squared_hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.01], loss [squared_hinge], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [1e-05], loss [perceptron], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.0001], loss [perceptron], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.001], loss [perceptron], penalty [l2]\n",
            "Training with hyperparameters: alpha [0.01], loss [perceptron], penalty [l2]\n",
            "Training with hyperparameters: alpha [1e-05], loss [perceptron], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.0001], loss [perceptron], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.001], loss [perceptron], penalty [l1]\n",
            "Training with hyperparameters: alpha [0.01], loss [perceptron], penalty [l1]\n",
            "Training with hyperparameters: alpha [1e-05], loss [perceptron], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.0001], loss [perceptron], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.001], loss [perceptron], penalty [elasticnet]\n",
            "Training with hyperparameters: alpha [0.01], loss [perceptron], penalty [elasticnet]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAH5NYLxbrD8",
        "outputId": "9b5d1424-aa63-472e-9a38-6b5b8f5436c2"
      },
      "source": [
        "#creating a pandas dataframe and sorting by accuracy      \n",
        "results = pd.DataFrame(results)\n",
        "results = results.sort_values(by='Accuracy', ascending=False)\n",
        "print(results)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Alpha     Penalty            Loss  Accuracy\n",
            "8   0.00001  elasticnet           hinge    84.615\n",
            "0   0.00001          l2           hinge    84.222\n",
            "20  0.00001  elasticnet  modified_huber    83.851\n",
            "4   0.00001          l1           hinge    83.318\n",
            "12  0.00001          l2  modified_huber    83.040\n",
            "13  0.00010          l2  modified_huber    82.901\n",
            "21  0.00010  elasticnet  modified_huber    82.298\n",
            "36  0.00001          l2      perceptron    81.974\n",
            "17  0.00010          l1  modified_huber    81.163\n",
            "1   0.00010          l2           hinge    80.561\n",
            "37  0.00010          l2      perceptron    80.329\n",
            "38  0.00100          l2      perceptron    79.819\n",
            "16  0.00001          l1  modified_huber    79.518\n",
            "25  0.00010          l2   squared_hinge    79.147\n",
            "33  0.00010  elasticnet   squared_hinge    78.313\n",
            "44  0.00001  elasticnet      perceptron    78.035\n",
            "9   0.00010  elasticnet           hinge    77.386\n",
            "5   0.00010          l1           hinge    77.317\n",
            "29  0.00010          l1   squared_hinge    77.132\n",
            "39  0.01000          l2      perceptron    76.854\n",
            "32  0.00001  elasticnet   squared_hinge    74.398\n",
            "24  0.00001          l2   squared_hinge    73.957\n",
            "28  0.00001          l1   squared_hinge    72.683\n",
            "40  0.00001          l1      perceptron    72.451\n",
            "26  0.00100          l2   squared_hinge    71.247\n",
            "14  0.00100          l2  modified_huber    71.200\n",
            "30  0.00100          l1   squared_hinge    69.926\n",
            "18  0.00100          l1  modified_huber    69.625\n",
            "22  0.00100  elasticnet  modified_huber    68.582\n",
            "45  0.00010  elasticnet      perceptron    68.582\n",
            "34  0.00100  elasticnet   squared_hinge    68.536\n",
            "46  0.00100  elasticnet      perceptron    65.361\n",
            "2   0.00100          l2           hinge    61.075\n",
            "10  0.00100  elasticnet           hinge    60.241\n",
            "6   0.00100          l1           hinge    60.195\n",
            "15  0.01000          l2  modified_huber    60.009\n",
            "27  0.01000          l2   squared_hinge    59.963\n",
            "35  0.01000  elasticnet   squared_hinge    59.430\n",
            "23  0.01000  elasticnet  modified_huber    59.407\n",
            "31  0.01000          l1   squared_hinge    58.781\n",
            "19  0.01000          l1  modified_huber    58.503\n",
            "3   0.01000          l2           hinge    58.272\n",
            "7   0.01000          l1           hinge    57.970\n",
            "11  0.01000  elasticnet           hinge    57.924\n",
            "41  0.00010          l1      perceptron    52.224\n",
            "43  0.01000          l1      perceptron    47.961\n",
            "47  0.01000  elasticnet      perceptron     0.185\n",
            "42  0.00100          l1      perceptron     0.070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiZSgrSrQcB1",
        "outputId": "57b60417-3613-46ec-8980-1dbcb23c2cd3"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Classifier\n",
        "classifier = OneVsRestClassifier(SGDClassifier(alpha=results.iloc[0]['Alpha'], penalty=results.iloc[0]['Penalty'], loss=results.iloc[0]['Loss'])) \n",
        "\n",
        "# train\n",
        "classifier.fit(X_train, y_train)\n",
        "#predict\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "#accuracy & time\n",
        "print(\"Total runtime %.2f seconds.\" % (time.time() - start_time))\n",
        "print(f\"Test accuracy is {results.iloc[0]['Accuracy']}.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total runtime 75.26 seconds.\n",
            "Test accuracy is 84.615.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-ZmnOVV6f5"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAG9BaVZQptT",
        "outputId": "819d3db7-9672-4337-8dc2-43a8b2af48b4"
      },
      "source": [
        "#Evaluating on the trained data\n",
        "predictions2 = classifier.predict(X_train)\n",
        "y_pred = mlb.inverse_transform(predictions2)\n",
        "print(round(accuracy_score(y_train,predictions2), 3)*100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf6yXWRcVxSz",
        "outputId": "4dc7fd47-99b6-47be-f886-d6356a37b152"
      },
      "source": [
        "#Evaluating on the test data\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print(f\"Test accuracy is {results.iloc[0]['Accuracy']}.\")\n",
        "print(\"==========\")\n",
        "\n",
        "precision_micro = precision_score(y_test, predictions,\n",
        "                            average='micro', labels=np.unique(predictions)) \n",
        "recall_micro = recall_score(y_test, predictions,\n",
        "                      average='micro', labels=np.unique(predictions))\n",
        "f1_micro = f1_score(y_test, predictions, average='micro', labels=np.unique(predictions))\n",
        " \n",
        "print(\"Micro-average:\")\n",
        "print(f'Precision: {round(precision_micro,3)}, Recall: {round(recall_micro,3)},\\\n",
        " F1-measure: {round(f1_micro,3)}')\n",
        "print(\"==========\")\n",
        " \n",
        "precision_macro = precision_score(y_test, predictions,\n",
        "                             average='macro',labels=np.unique(predictions))\n",
        "recall_macro = recall_score(y_test, predictions,\n",
        "                      average='macro', labels=np.unique(predictions))\n",
        "f1_macro = f1_score(y_test, predictions, average='macro', labels=np.unique(predictions))\n",
        " \n",
        "print(\"Macro-average:\")\n",
        "print(f'Precision: {round(precision_macro,3)}, Recall: {round(recall_macro,3)},F1-measure: {round(f1_macro,3)}')\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 84.615.\n",
            "==========\n",
            "Micro-average:\n",
            "Precision: 0.865, Recall: 0.879, F1-measure: 0.871\n",
            "==========\n",
            "Macro-average:\n",
            "Precision: 0.87, Recall: 0.852,F1-measure: 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfp0lqXMxvr"
      },
      "source": [
        "# Probabilistic classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0g-8Rq_NgKP"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSkAfdo3XD4o",
        "outputId": "06d6dc80-2bb9-4fc6-8dcd-9d04d7c79811"
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#looking for the best hyperparameters\n",
        "alpha_parameters = [1e-05, 0.0001, 0.001, 0.01, 0.1, 1]\n",
        "\n",
        "results = []\n",
        "\n",
        "for alpha in alpha_parameters:\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    classifier = OneVsRestClassifier(MultinomialNB(alpha=alpha))\n",
        "\n",
        "    # train\n",
        "    classifier.fit(X_train, y_train)\n",
        "    print(f'Training with hyperparameters: alpha [{alpha}]')\n",
        "    \n",
        "    #predict\n",
        "    predictions = classifier.predict(X_test)\n",
        "\n",
        "    #accuracy\n",
        "    accuracy = accuracy_score(y_test, predictions)*100\n",
        "\n",
        "    #dataframe\n",
        "    results.append({\n",
        "            'Alpha': alpha,\n",
        "            'Accuracy': round(accuracy, 3)\n",
        "              })\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: alpha [1e-05]\n",
            "Training with hyperparameters: alpha [0.0001]\n",
            "Training with hyperparameters: alpha [0.001]\n",
            "Training with hyperparameters: alpha [0.01]\n",
            "Training with hyperparameters: alpha [0.1]\n",
            "Training with hyperparameters: alpha [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyMrETXPcoos",
        "outputId": "fc2c9d15-a8b0-4247-ad3e-78988e6b9501"
      },
      "source": [
        "#creating a pandas dataframe and sorting by accuracy      \n",
        "results = pd.DataFrame(results)\n",
        "results = results.sort_values(by='Accuracy', ascending=False)\n",
        "print(results)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Alpha  Accuracy\n",
            "2  0.00100    77.363\n",
            "1  0.00010    77.294\n",
            "0  0.00001    77.201\n",
            "3  0.01000    76.622\n",
            "4  0.10000    66.311\n",
            "5  1.00000    61.098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a71c4M2-W2Qz",
        "outputId": "ec308e3d-098e-47c3-e306-ef0a0e5a9432"
      },
      "source": [
        "#training and testing with the best alpha\n",
        "\n",
        "classifier = OneVsRestClassifier(MultinomialNB(alpha=results.iloc[0]['Alpha']))\n",
        "\n",
        "# train\n",
        "classifier.fit(X_train, y_train)\n",
        "#predict\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# accuracy\n",
        "print (\"Total runtime %.2f seconds.\" % (time.time() - start_time))\n",
        "print(f\"Test accuracy is {results.iloc[0]['Accuracy']}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total runtime 434.72 seconds.\n",
            "Test accuracy is 77.363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z7gX0oQNDOC"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ6RNxxRT7Xv",
        "outputId": "f1c7625d-dc76-4cf3-cbd4-26ab42c93645"
      },
      "source": [
        "#Evaluating on the trained data\n",
        "predictions2 = classifier.predict(X_train)\n",
        "y_pred = mlb.inverse_transform(predictions2)\n",
        "print(round(accuracy_score(y_train,predictions2), 3)*100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryVANj43W8HV",
        "outputId": "57bc409e-0ef7-4404-a62e-d9f0e2766c3d"
      },
      "source": [
        "#Evaluating on the test data\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print(f\"Test accuracy is {results.iloc[0]['Accuracy']}.\")\n",
        "print(\"==========\")\n",
        "\n",
        "precision_micro = precision_score(y_test, predictions,\n",
        "                            average='micro', labels=np.unique(predictions)) \n",
        "recall_micro = recall_score(y_test, predictions,\n",
        "                      average='micro', labels=np.unique(predictions))\n",
        "f1_micro = f1_score(y_test, predictions, average='micro', labels=np.unique(predictions))\n",
        " \n",
        "print(\"Micro-average:\")\n",
        "print(f'Precision: {round(precision_micro,3)}, Recall: {round(recall_micro,3)},\\\n",
        " F1-measure: {round(f1_micro,3)}')\n",
        "print(\"==========\")\n",
        " \n",
        "precision_macro = precision_score(y_test, predictions,\n",
        "                             average='macro',labels=np.unique(predictions))\n",
        "recall_macro = recall_score(y_test, predictions,\n",
        "                      average='macro', labels=np.unique(predictions))\n",
        "f1_macro = f1_score(y_test, predictions, average='macro', labels=np.unique(predictions))\n",
        " \n",
        "print(\"Macro-average:\")\n",
        "print(f'Precision: {round(precision_macro,3)}, Recall: {round(recall_macro,3)},F1-measure: {round(f1_macro,3)}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 77.363.\n",
            "==========\n",
            "Micro-average:\n",
            "Precision: 0.795, Recall: 0.777, F1-measure: 0.786\n",
            "==========\n",
            "Macro-average:\n",
            "Precision: 0.854, Recall: 0.715,F1-measure: 0.773\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}